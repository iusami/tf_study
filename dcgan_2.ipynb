{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel2/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "#from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            \n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig('gen_imgs/fashion_mnist_%d.png' % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 1.189270, acc.: 34.38%] [G loss: 0.583436]\n",
      "1000 [D loss: 1.047249, acc.: 50.00%] [G loss: 0.377348]\n",
      "2000 [D loss: 0.952667, acc.: 50.00%] [G loss: 0.328376]\n",
      "3000 [D loss: 0.852995, acc.: 50.00%] [G loss: 0.403615]\n",
      "4000 [D loss: 0.851421, acc.: 51.56%] [G loss: 0.429540]\n",
      "5000 [D loss: 0.980892, acc.: 48.44%] [G loss: 0.444079]\n",
      "6000 [D loss: 0.895615, acc.: 50.00%] [G loss: 0.398776]\n",
      "7000 [D loss: 0.941683, acc.: 51.56%] [G loss: 0.391681]\n",
      "8000 [D loss: 0.971205, acc.: 54.69%] [G loss: 0.403958]\n",
      "9000 [D loss: 1.191676, acc.: 45.31%] [G loss: 0.396745]\n",
      "10000 [D loss: 1.230589, acc.: 53.12%] [G loss: 0.285364]\n",
      "11000 [D loss: 1.258391, acc.: 48.44%] [G loss: 0.414858]\n",
      "12000 [D loss: 1.300349, acc.: 48.44%] [G loss: 0.506726]\n",
      "13000 [D loss: 1.098388, acc.: 54.69%] [G loss: 0.548330]\n",
      "14000 [D loss: 1.441617, acc.: 50.00%] [G loss: 0.308241]\n",
      "15000 [D loss: 1.359912, acc.: 48.44%] [G loss: 0.480401]\n",
      "16000 [D loss: 1.376748, acc.: 45.31%] [G loss: 0.255854]\n",
      "17000 [D loss: 0.816925, acc.: 59.38%] [G loss: 0.673775]\n",
      "18000 [D loss: 1.232661, acc.: 50.00%] [G loss: 0.308539]\n",
      "19000 [D loss: 1.401014, acc.: 53.12%] [G loss: 0.615690]\n",
      "20000 [D loss: 0.977125, acc.: 54.69%] [G loss: 0.459145]\n",
      "21000 [D loss: 1.400850, acc.: 43.75%] [G loss: 0.928792]\n",
      "22000 [D loss: 0.999464, acc.: 62.50%] [G loss: 0.357003]\n",
      "23000 [D loss: 1.517386, acc.: 43.75%] [G loss: 0.424290]\n",
      "24000 [D loss: 1.272696, acc.: 54.69%] [G loss: 0.597172]\n",
      "25000 [D loss: 1.122695, acc.: 50.00%] [G loss: 0.452701]\n",
      "26000 [D loss: 1.378021, acc.: 59.38%] [G loss: 0.620139]\n",
      "27000 [D loss: 0.557141, acc.: 76.56%] [G loss: 1.049794]\n",
      "28000 [D loss: 1.287018, acc.: 48.44%] [G loss: 0.221238]\n",
      "29000 [D loss: 1.201710, acc.: 45.31%] [G loss: 0.210954]\n",
      "30000 [D loss: 0.972927, acc.: 48.44%] [G loss: 0.393983]\n",
      "31000 [D loss: 0.561865, acc.: 76.56%] [G loss: 0.673145]\n",
      "32000 [D loss: 1.386619, acc.: 43.75%] [G loss: 0.264164]\n",
      "33000 [D loss: 0.993130, acc.: 59.38%] [G loss: 0.178853]\n",
      "34000 [D loss: 1.485661, acc.: 59.38%] [G loss: 0.380083]\n",
      "35000 [D loss: 1.168999, acc.: 51.56%] [G loss: 0.236007]\n",
      "36000 [D loss: 1.144760, acc.: 53.12%] [G loss: 0.169315]\n",
      "37000 [D loss: 1.420734, acc.: 50.00%] [G loss: 0.722890]\n",
      "38000 [D loss: 1.460948, acc.: 46.88%] [G loss: 0.195476]\n",
      "39000 [D loss: 1.135675, acc.: 45.31%] [G loss: 0.197327]\n",
      "40000 [D loss: 1.091592, acc.: 57.81%] [G loss: 0.257735]\n",
      "41000 [D loss: 1.667584, acc.: 51.56%] [G loss: 0.232700]\n",
      "42000 [D loss: 1.556053, acc.: 46.88%] [G loss: 0.291525]\n",
      "43000 [D loss: 1.285477, acc.: 46.88%] [G loss: 0.211545]\n",
      "44000 [D loss: 1.791145, acc.: 48.44%] [G loss: 0.324801]\n",
      "45000 [D loss: 1.380685, acc.: 50.00%] [G loss: 0.202713]\n",
      "46000 [D loss: 1.116060, acc.: 48.44%] [G loss: 0.763915]\n",
      "47000 [D loss: 1.310775, acc.: 54.69%] [G loss: 0.638567]\n",
      "48000 [D loss: 1.441291, acc.: 51.56%] [G loss: 0.142473]\n",
      "49000 [D loss: 1.652097, acc.: 57.81%] [G loss: 0.209040]\n",
      "50000 [D loss: 1.849088, acc.: 48.44%] [G loss: 0.170009]\n",
      "51000 [D loss: 1.166221, acc.: 53.12%] [G loss: 0.620206]\n",
      "52000 [D loss: 1.040861, acc.: 64.06%] [G loss: 0.223476]\n",
      "53000 [D loss: 1.555556, acc.: 57.81%] [G loss: 0.226612]\n",
      "54000 [D loss: 1.443609, acc.: 50.00%] [G loss: 0.725332]\n",
      "55000 [D loss: 1.603981, acc.: 46.88%] [G loss: 0.252690]\n",
      "56000 [D loss: 1.747580, acc.: 46.88%] [G loss: 0.222973]\n",
      "57000 [D loss: 1.848875, acc.: 43.75%] [G loss: 0.491558]\n",
      "58000 [D loss: 1.424672, acc.: 45.31%] [G loss: 0.398223]\n",
      "59000 [D loss: 1.291258, acc.: 50.00%] [G loss: 0.456678]\n",
      "60000 [D loss: 1.602612, acc.: 46.88%] [G loss: 0.257747]\n",
      "61000 [D loss: 1.460784, acc.: 50.00%] [G loss: 0.186367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62000 [D loss: 2.057195, acc.: 43.75%] [G loss: 0.157491]\n",
      "63000 [D loss: 1.550592, acc.: 50.00%] [G loss: 0.362000]\n",
      "64000 [D loss: 1.473592, acc.: 56.25%] [G loss: 0.343870]\n",
      "65000 [D loss: 1.247605, acc.: 50.00%] [G loss: 0.237093]\n",
      "66000 [D loss: 1.263654, acc.: 59.38%] [G loss: 0.258646]\n",
      "67000 [D loss: 1.630831, acc.: 46.88%] [G loss: 0.666750]\n",
      "68000 [D loss: 1.385939, acc.: 48.44%] [G loss: 0.281859]\n",
      "69000 [D loss: 0.801430, acc.: 71.88%] [G loss: 0.270345]\n",
      "70000 [D loss: 1.686473, acc.: 50.00%] [G loss: 0.150625]\n",
      "71000 [D loss: 1.290154, acc.: 53.12%] [G loss: 0.502495]\n",
      "72000 [D loss: 0.523660, acc.: 79.69%] [G loss: 0.274018]\n",
      "73000 [D loss: 1.490916, acc.: 48.44%] [G loss: 0.351525]\n",
      "74000 [D loss: 0.965430, acc.: 67.19%] [G loss: 0.267754]\n",
      "75000 [D loss: 1.561338, acc.: 46.88%] [G loss: 0.227819]\n",
      "76000 [D loss: 0.965927, acc.: 65.62%] [G loss: 0.646193]\n",
      "77000 [D loss: 1.706739, acc.: 43.75%] [G loss: 0.477846]\n",
      "78000 [D loss: 1.123092, acc.: 57.81%] [G loss: 0.182828]\n",
      "79000 [D loss: 1.713378, acc.: 48.44%] [G loss: 0.231232]\n",
      "80000 [D loss: 2.011792, acc.: 42.19%] [G loss: 0.264399]\n",
      "81000 [D loss: 1.008070, acc.: 59.38%] [G loss: 0.306442]\n",
      "82000 [D loss: 1.557781, acc.: 48.44%] [G loss: 0.290090]\n",
      "83000 [D loss: 1.544281, acc.: 51.56%] [G loss: 0.200481]\n",
      "84000 [D loss: 2.151772, acc.: 43.75%] [G loss: 0.216754]\n",
      "85000 [D loss: 1.668120, acc.: 53.12%] [G loss: 0.117485]\n",
      "86000 [D loss: 1.739339, acc.: 39.06%] [G loss: 0.229700]\n",
      "87000 [D loss: 1.787996, acc.: 45.31%] [G loss: 0.194977]\n",
      "88000 [D loss: 1.448621, acc.: 53.12%] [G loss: 0.383014]\n",
      "89000 [D loss: 1.916248, acc.: 37.50%] [G loss: 0.322762]\n",
      "90000 [D loss: 1.629692, acc.: 48.44%] [G loss: 0.436990]\n",
      "91000 [D loss: 1.733558, acc.: 46.88%] [G loss: 0.213791]\n",
      "92000 [D loss: 1.532739, acc.: 39.06%] [G loss: 0.372324]\n",
      "93000 [D loss: 0.722011, acc.: 73.44%] [G loss: 0.179745]\n",
      "94000 [D loss: 1.536498, acc.: 53.12%] [G loss: 0.298812]\n",
      "95000 [D loss: 1.549123, acc.: 40.62%] [G loss: 0.219324]\n",
      "96000 [D loss: 1.296257, acc.: 57.81%] [G loss: 0.332242]\n",
      "97000 [D loss: 1.933994, acc.: 46.88%] [G loss: 0.251637]\n",
      "98000 [D loss: 1.422663, acc.: 48.44%] [G loss: 0.288868]\n",
      "99000 [D loss: 1.160923, acc.: 45.31%] [G loss: 0.128863]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=100000, batch_size=32, save_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
